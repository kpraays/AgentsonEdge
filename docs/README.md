# Experiments

1. **Category-Wise Query Submission**:
    - **Process**: We submitted 100 queries from each category sequentially. Every 0.5 seconds, the following metrics were recorded:
        1. **CPU and GPU Memory Usage, CPU Utilization**: These metrics were recorded (across all four cores: CPU utilization) for the query process executed through llama.cpp for TinyLlama.
        2. **System-Wide GPU Utilization**: GPU utilization was monitored while the query execution process was in progress. During data processing, the GPU utilization values used for plots were those that coincided with the same time periods for which per-process readings existed.
        3. **Per Query Metrics**:
            1. **Time to First Token**: The duration before the first token appeared.
            2. **Time Between Tokens**: The time gap observed between the generation of each successive token after the first token had appeared.
            
2. **Granular Query Analysis**:
    - **Process**: We calculated the median length of prompt tokens for each query category and selected one representative query from each category that closely matched the median token length. For each representative query, the same measurements as above were recorded every centisecond (1 centisecond equals 1/100th of a second, corresponding to 100 ticks per second on Ubuntu systems). Recordings were made after a warm-up phase, as discrepancies were found in the CPU usage of complex queries when measurements were taken without warming up. The initial four readings for other queries in each workload were discarded to ensure consistency.

[*Median prompt lengths across categories.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdDsqeY3ClqG_fDQicxfLzuX1zX5xwhWpAcpxSiYTZnc9zvKma5ICHlR-ECoj2nvaa7mI02JmSp27XfI2ZdtCqR8OnpBVB3nt8FHZ_Yv_8o2iWfC8JzjSx7X9oib_-WL_9mbv2owEj0WH-CHvSoSJvlr9wk?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Median prompt lengths across categories.*

**Additional Granular Measurements for Special Scenarios**:

For each of the following scenarios, a warm-up phase was included to ensure consistency in measurements.

1. **Task-Oriented Queries**:
    - **Process**: GPU memory usage was recorded every centisecond for four task-oriented queries. These queries were selected based on an increasing number of tokens in the prompt, with the output token count fixed at 50.
2. **Total Time Measurement**:
    - **Process**: The total time taken to complete two queries was recorded—one from the task-oriented workload and one from the contextual workload. These queries were selected to minimize the difference in the number of tokens in the prompt between them.
3. **Conversational Workload Query**:
    - **Process**: A single conversational query with a number of prompt tokens equal to the category median was selected. GPU memory usage was recorded over five runs of the same query, with the expected output length increasing incrementally in each run. The output token counts were set to ["10", "50", "120", "230", "400"].




# Analysis

## Understanding Resource Usage in LLMs

To understand the trends observed in the experiments defined above, it's important to grasp how outputs are generated by LLMs. 

### **Encoder-Decoder Models**

Encoder-decoder models convert input sequences into latent hidden representations, which they use to generate output responses. This makes them well-suited for tasks like information retrieval and question answering. The process begins with the tokenization of the input prompt, which is then converted into word embeddings based on the model's vocabulary. These embeddings are augmented with positional information, resulting in positional encodings that serve as the input for the encoder blocks. (These embeddings can be moved to CPU memory to free up accelerators for attention operations.)

Within each encoder layer, self-attention operations capture the relationships between tokens in a sentence. This involves calculating how much influence other tokens have on a particular token and scaling the numeric representation of that token by the probability of that influence. To achieve this, three quantities are used: queries (Q), keys (K), and values (V), which are computed by applying weight matrices ($W^Q, W^K, W^V$) to the input X (the encoded representation of the input text sequence) ([LLM inference serving](https://arxiv.org/pdf/2407.12391)).

In models with multi-head self-attention (MHSA), multiple sets of weights for keys and values operate on the same set of tokens per layer. The MHSA block combines features from all attention heads to form its output by multiplying it with another weight matrix, which is also trained with the rest of the system. All these matrices need to be in memory for computation. To maximize the use of hardware accelerators, it's crucial to maintain the state of multiple sets of matrices in the accelerator memory, avoiding transfer delays and idling.

The combined representation of captured self-attention information is normalized and merged back with positional embeddings (residuals) to prevent vanishing and exploding gradients before passing through a feedforward layer. This residual process is repeated with normalization within each encoder block. Different models may have multiple stacked encoder blocks (layers). The final layer provides self-attention values for input tokens, capturing the semantic context, positional ordering, and relationships between tokens in the input.

The decoding phase in an encoder-decoder model utilizes the attention values from the final encoder layer (essentially the higher latent representation of the input data) to generate outputs at each successive decoder layer. This allows the decoder to track significant words in the input. Additionally, because the decoder doesn't know the full output sequence, it applies masked self-attention (explained in decoder section) to record the influence of previous tokens.

### **Decoder-Only Models**

In decoder-only LLMs, output tokens are generally generated sequentially based on the initial input sequence (prompt). These models have a single unit for both encoding the input and generating the output. They use masked self-attention throughout all their layers. The input processing, or prefill stage, is highly parallel, where inputs are embedded and encoded with multi-head attention (steps include tokenization, embeddings, positional embeddings, self-attention values, and normalization with residuals). This stage involves large matrix multiplications that utilize hardware accelerators. Multi-head attention allows the decoder to consider different parts of the sequence in different representational spaces ([LLM inference serving](https://arxiv.org/pdf/2407.12391)).

While calculating self-attention values masked self-attention is used as the decoder is not yet aware about the tokens from the ungenerated sequence. So, only the decoder layers only consider the previous and current tokens when calculating self-attention. This process is repeated over multiple decoder layers until the final decoder outputs are produced. To avoid recomputing the keys and values for the input token while generating the output, the key-value (KV) pairs for previous input tokens can be stored in the accelerator RAM through a KV cache. This cache consumes significant memory during execution but is necessary for maintaining usable inference speed.

The second phase, decoding, involves the autoregressive generation of new tokens by the model, with updates to the KV cache for each new token. The process continues until an `<end of string>` token is encountered or the maximum output length is reached. This is the core part of the inference process.

To convert the decoder layer's values into text, a linear layer is used to map them to the model's vocabulary dimensions, followed by a softmax function to obtain a probability distribution over the vocab tokens, generating the final output.

In its simplest form, the model generates output one token at a time, using the generated tokens along with the existing ones as the input sequence to generate the rest. This approach has limited parallelism compared to encoders.

**Resource Consumption Considerations:**

The encoding phase is highly parallelizable due to independent token processing and the computationally intensive operations on keys, values, and queries. GPUs, with their parallel architecture and dedicated memory, are well-suited for accelerating these computations. However, frequent data transfer between CPU and GPU memory can introduce performance overheads. Consequently, a hybrid approach is often employed, utilizing the GPU for intensive calculations and the CPU for auxiliary tasks and data management. The overall resource consumption of the encoding phase is determined by the model's size, the number of layers, and the computational complexity of the operations performed at each layer.

### TinyLlama: A Specialized Decoder-Only Model

TinyLlama ([TinyLlama: An Open-Source Small Language Model](https://arxiv.org/pdf/2401.02385)) is a decoder-only transformer model based on the architecture of the Llama series. Despite having only 1.1 billion parameters, TinyLlama stands out because it has been trained on an impressive 3 trillion tokens, going against the [Chinchilla](https://arxiv.org/abs/2203.15556) scaling laws. This extensive training dataset allows TinyLlama to achieve significant performance even with a relatively smaller model size.

TinyLlama uses Rotary Positional Embedding (RoPE) ([Su et al., 2021](https://arxiv.org/abs/2104.09864)) to capture positional information. Unlike traditional transformers, where normalization occurs after each transformer layer, TinyLlama, following Llama's architecture, normalizes inputs before each transformer layer using Root Mean Square Layer Normalization ([RMSNorm](https://github.com/bzhangGo/rmsnorm)). RMSNorm has been shown to improve efficiency by 10-50% without a significant decrease in performance ([llama-2-from-the-ground-up](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)).

Similar to the Llama series, TinyLlama uses the SwiGLU activation function instead of the more commonly used ReLU. Additionally, it employs Grouped-query attention, which helps reduce memory bandwidth and speeds up inference. In Grouped-query attention, keys and values can be shared between multiple heads, requiring less memory space than traditional multi-head attention. Specifically, TinyLlama has 32 heads for query attention and 4 groups for key-value heads ([TinyLlama: An Open-Source Small Language Model](https://arxiv.org/pdf/2401.02385)).

### Resource usage contributors

Model size: The number of parameters of an LLM is the approximate total number of weights and biases across all layers that are being used in that model. Thus, the number of weights and the precision in which they are stored will directly influence the space the model takes when loaded in the memory ([Zhang et al.](https://arxiv.org/pdf/2404.14294)).

Attention Operation: As the input length becomes longer, the memory and computational costs associated with attention operation increase quickly because it exhibits quadratic computational complexity in the input length ([Zhang et al.](https://arxiv.org/pdf/2404.14294)).

Decoding Approach: Decoding requires loading KV cache in memory on the accelerator. Though, it happens step by step per token but this cache needs to be in memory for each step and KV cache grows in size as well. This may lead to fragmented memory and irregular memory access patterns ([Zhang et al.](https://arxiv.org/pdf/2404.14294)).

## Analysis of plots across workloads

[*Analysis of CPU and GPU Memory Usage across workloads.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXebf6N7ikqQT9joraEkvZIWiw56IRDB1Z1I9kU3ze5pLoEFFAyQua9YumGQwHReufR0PgzOLZd7FoOp7Yk4heMZ3qgtY7P7G896L2w2MKB_Sp4v-m2SCuV792Zpcop12OvKW0-d0seBr6-gqj4OCELfiqYX?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Analysis of CPU and GPU Memory Usage across workloads.*

### Analysis of Memory usage

We observed that CPU memory usage remains relatively consistent across different workloads, even when the length of queries varies. This consistency is likely due to the storage of embeddings in CPU memory during inference, as these embeddings are required either at the start of the computation for each query or at the end, to convert token IDs back to text from individual probabilities. Since the size of these embeddings does not significantly vary between workloads, the CPU memory usage remains fairly constant. The primary computational load, particularly for attention operations, occurs on the GPU. When the GPU memory becomes overwhelmed, we observe some data being offloaded to CPU memory. This is evident in workloads such as contextual and task-oriented queries, which exhibit higher than average GPU memory usage, accompanied by a slight increase in CPU memory usage, as shown in the zoomed-in version of the CPU memory usage plot.

[*Zoomed in CPU memory usage across workloads.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdWQToxlQ0vU8r9EzMA1RvZcSXkbMmIrNOhaVrvfC7xvfvO77yG_-4Kop0kG6RRtXSp5HrSZYsBdrUlN1XfgCVVqWDSl4XYPPrfdJHXOfiz3_ckAQnxa1q-6SSyJjXH6iCyRn3x_N4N7UCLSoInQC6Jyb9P?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Zoomed in CPU memory usage across workloads.*

Regarding GPU memory usage, the contextual workload stands out with significantly higher memory consumption, while the task-oriented workload shows slight spikes compared to others. This difference can be attributed to the storage of weights, biases, and the KV cache in GPU memory during inference. The KV cache is constructed during the prefill stage and updated with each new token generated during the decoding phase, allowing subsequent tokens to utilize previous KV entries for computing attention. Consequently, the size of the KV cache is directly proportional to the number of tokens, leading to increased memory usage for longer prompts. The average prompt length for queries in the contextual workload is notably longer than in other workloads, with the task-oriented workload following closely behind.

[*Mean prompt length across categories.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdG8nETUTwXJg_5aKV6Hzjd4xpSmvZrMde4DVtPQmx7PpDuENSCfqFqeoHZPRIdNldJ01vDPBdzhtOGuoquYUi8LIFNtwWDxUAbebvlKxHBFfCzUqbeIvnsTG8KvORQ3qYwq846giHveJz9Q7FbB6blexSK?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Mean prompt length across categories.*

### Analysis of Compute

[*Average CPU and GPU usage across workloads.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdlnVLKGLGEGZVe_5FJqQOMeIn_mKWRWYe1c0plGNLfny7EWTuZQYG0zJHE8oEG38QJIhlxKJBJIz1_l1TRKEMtrWiq2cuwPNEZklIB-rIH4Z5ib91Rp2X3jgdkIkcwTP7C2yl8vngmzr818gZwz1ENfgK_?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Average CPU and GPU usage across workloads.*

We observed that average CPU usage is lower for conversational workloads compared to other types. This may be attributed to the TinyLlama model being specifically fine-tuned for chat-related use cases. If conversational patterns were better represented during the model's training stages, the model might be more efficient in handling these workloads, leading to quicker generation times. The near-full GPU utilization observed for conversational workloads could indicate that the GPU is managing these tasks more efficiently, with less need for the CPU to handle intermediate states. Similarly, the simple workload also shows relatively low average CPU usage, likely because these queries are straightforward and recall-based, requiring minimal CPU intervention to manage intermediate states on the GPU.

We also noted significant variation in GPU usage across complex and simple workloads, with noticeable drops in GPU memory usage for these categories, while conversational workloads consistently occupy GPU memory. Fluctuations in GPU usage may occur when the model needs to reference different parts of its knowledge base. Complex queries often demand multiple layers of reasoning and context processing, whereas simple queries typically require straightforward recall, triggering fewer deep transformer layers. This difference is reflected in the GPU usage plot, where the simple workload shows fewer spikes in GPU utilization compared to the complex workload. The same behavior is observed in GPU memory usage. An interesting insight is that some queries in the simple workload may be similar in sentence length and information requirements to those in the complex workload, as suggested by the observed patterns in GPU usage.

The GPU is heavily utilized when numerous parallel operations are required, particularly during token generation (e.g., attention computation, normalization, and feedforward layers—essentially matrix multiplication). The consistently high GPU usage for contextual and conversational workloads indicates that these tasks are more generation-intensive than others. This is intuitive, as the GPU performs significant work in establishing attention with existing tokens in contextual workloads, where queries are generally longer. For conversational workloads, the model's fine-tuning for chat-related queries might result in greater GPU efficiency or utilization, explaining why task-oriented workloads do not exhibit consistently high GPU usage, despite involving step-by-step generation based on user utterances.

Finally, the plots for both memory usage and compute illustrate that different workload types require varying amounts of time to execute, largely due to the expected number of output tokens. Task-oriented workloads, which have an expected output of 100 tokens—the highest among all categories—naturally take longer to generate their output.

***Limits on number of output tokens:***

```python
*{"complex": "70", 
"contextual": "50", 
"conversational": "40", 
"simple": "30", 
"task-oriented": 
"100"}*
```

[*Time to First token (TTFT) and Time between Tokens (TBT) across workloads.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdBoA5EjbDx3qnv--nFOhtp6BS-iOxMtysBOu9oDBkbo3PAeoY-OkU8MZLOGus_3mlltGx02wOiO64QG2XfO9LbEeeNCyOIfHw6T25ah2JDH6fMo8CK2ZcrOGwAqGFZJ274LIvb_OcGYgJuV9C0KjgwEnnC?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Time to First token (TTFT) and Time between Tokens (TBT) across workloads.*

### Latency Analysis

We measured latency in two forms: the time to generate the first output token for a query, referred to as "Time to First Token" (TTFT), and the latency between generating each subsequent token during the decode stage, referred to as "Time Between Tokens" (TBT).

**Time to First Token (TTFT)**:

For workloads with longer prompts, such as contextual and task-oriented queries, we expected TTFT to be greater than for other workloads, which is consistent with our observations. We noticed a larger variation in TTFT for task-oriented workloads compared to others, likely due to the wider range of token counts in the prompts of task-oriented queries. TTFT is heavily influenced by the prefill stage of LLM inference, where the length of the prompt and the time required to build the KV cache directly affect TTFT. An interesting observation is that conversational queries exhibit the least variation in TTFT and have a shorter TTFT compared to simple and complex workloads, despite not being shorter in length. This could be attributed to TinyLlama being specifically fine-tuned for chat instructions, which may enhance its efficiency in handling conversational prompts.

**Time Between Tokens (TBT)**:

TBT essentially represents the time it takes to execute one step of the decoding cycle given the existing sequence. During this stage, the KV cache, which was built and loaded into memory during the prefill stage, must be updated for each new token. Additionally, other operations, such as matrix calculations for normalization and the feedforward layer, are performed. A longer TBT suggests that the GPU is taking more time to complete these operations, potentially indicating that the GPU is compute-bound or that GPU memory is insufficient, leading to data being swapped between CPU and GPU memory, causing delays.

In our analysis, we observed a slight increase in CPU memory usage for contextual and task-oriented workloads, which could suggest some level of data transfer between CPU and GPU memory. However, given that these spikes are minimal, the delays are likely due to the GPU being compute-bound rather than memory-constrained. This indicates that the contextual and task-oriented workloads place a higher strain on the GPU compared to other workloads, as expected.

[*Total time per query vs Tokens in Prompt across workloads.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdFQu0nlDHmRAc8lwJy2aU46CabTUX7Hpg-i08aDRklXmQDh_FAfBxTGMaqPhvAFbG_6vRCc2TeEGyQs3V3-7AcEET91XITW0qN12MoRRLulQyfkABNkiy24L5e8Belp_1Kjtb_WGRexmHVJBXY3OhxZ1s?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Total time per query vs Tokens in Prompt across workloads.*

### Analysis of Total Execution Time for Queries

Task-oriented queries exhibit the largest variation in prompt size compared to other workloads. We anticipated that an increase in the number of tokens in the prompt would result in a longer total execution time for a query, primarily due to the expected increase in TTFT. Additionally, a higher number of expected output tokens would naturally extend the total execution time, as more tokens need to be generated.

In the plot, an increasing trend in "total time for query" is evident within each workload as the number of tokens in the prompt increases. Task-oriented workloads show elevated values for "total time taken for query," which can be attributed to the higher expected output token count in these queries compared to other workloads. However, we also observe variations in the total time taken for queries within the same workload, even when the number of prompt tokens is similar.

These variations may be influenced by how each individual query interacts with the model, particularly in terms of recalling information from deeper layers, reasoning, understanding context, and the GPU's role in generating tokens. The complexity and nature of each query likely contribute to differences in execution time, even within the same workload category.

## Granular analysis of Queries

[*Average CPU and GPU use for a single query.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcuMks2bU6vXiSoKY3cQggGxxEklcTRkyJgf3eYtjCHX6FZ8S5DpN_rQolEObfNaZdySOFDytaY3EUcP1wvDzv6zbujGofefmkSr4cpp1wfF_rf8z7NuL37dKK4wnmaj5bdyMiNm4HXx2TDCYTFvN7t4T4?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Average CPU and GPU use for a single query.*

### Analysis of CPU and GPU Usage Patterns

Our analysis shows that the average CPU usage across different queries remains fairly consistent (excluding measurements from the warm-up phase). However, we observed that task-oriented and contextual queries exhibit two or more GPU usage spikes—typically at the beginning and the end of the query—whereas other query types generally show a spike only at the beginning.

This behavior can be attributed to the nature of these workloads. In contextual queries, the prompt usually includes a paragraph providing context, followed by a question related to that context. For task-oriented queries, the model is asked to function as a bot, assisting the user with specific tasks such as booking movies, hotels, or flights. These queries often involve a sequence of <USER> utterances, requiring the model to understand the task and generate a step-by-step plan for its completion.

As a result, queries in these workloads are generally longer than those in other categories and require the model to deliver output after processing and understanding the context. The multiple GPU usage spikes likely indicate heavy computation occurring in two or more phases. In the first phase, the model might be establishing the context by filling up the KV cache, which is relatively larger than in other workloads. In the second or subsequent phases, the model performs iterative processing to generate responses based on the previously established context.

This pattern is particularly interesting because it suggests that even other workloads (except for simple queries) show smaller subsequent GPU usage spikes, indicating that the model requires initial GPU computation to establish context and additional GPU resources later to generate responses. Although this is a decoder-only model, which typically does not exhibit phase differentiation in GPU usage as encoder-decoder models do, the observed behavior suggests that similar phase-dependent GPU usage patterns may still occur during inference.

[*CPU and GPU memory usage for individual queries.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdJHwNDSxjJfF9RDeW5R19P8Mo-OQGXvXlXuoXmmO7lWak9TdgRlY29ZPshZerRHRtsvbWIVEWMUH4tEMqmn9W_v_AVuHwhOjs941BitACO0xPZfABt8m4aENtO270f2y-E45b3NoPpjbxYi3oPT9aDRmQe?key=7VKbPIlbOp2wxPlxUTA4Ug)

*CPU and GPU memory usage for individual queries.*

### Granular memory usage analysis

**Components Contributing to CPU Memory Usage**

The following components are stored in CPU memory during inference, based on observations from the query logs generated by llama.cpp:

1. **Model Weights**: The model weights are stored in CPU memory. Since these weights are consistent across all query types, this factor remains constant.
2. **Embeddings for Input Tokens**: The embeddings, which map each input token to a high-dimensional vector space, are computed and stored in CPU memory. The embedding layer is the same for all query types, making this a consistent factor across workloads.
3. **KV Cache Allocation**: A fixed portion of the KV cache is allocated in CPU memory, determined by the maximum context length, the number of layers, and the number of heads. This allocation does not vary with the query type.
4. **Pre-Allocated Input and Compute Buffers**: These buffers store intermediate data before it is transferred to the GPU. They remain the same regardless of the query type.

**Components Contributing to GPU Memory Usage**

More complex or lengthy queries, which involve long contexts, more reasoning, and less recall-based output, tend to occupy more GPU memory. This is especially true for contextual, conversational, and task-oriented workloads, as not all queries from complex and simple workloads exhibit these characteristics. The key components taking up GPU memory include:

1. **Temporary Tensor Cores**: GPU memory is required to store temporary tensor cores for operations such as matrix multiplications and attention calculations. Although these tensors are often deallocated after each operation, they can occupy significant space during execution. More complex queries, such as those in contextual or task-oriented workloads, generate more intermediate results, requiring more temporary tensors and thus higher GPU memory usage.
2. **KV Cache**: The KV cache primarily resides in the GPU for faster memory access, supporting attention calculations during the token generation process. While the size of this cache is influenced by the actual context length, it remains fairly consistent across queries, given the absence of an order of magnitude difference in context length.
    - Estimating KV cache usage ([Verma](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)) as:

> **Total size of KV cache in bytes = *2 (two matrices - K,V) * (context length) * (number of heads) * (embedding dimension per head) * (number of layers) *  2 Bytes (sizeof fp16)***
> 

Consider a context length of 512 (on the higher side) across all workloads, we get KV cache size as 

$$
2 * (512) * (32) * (64) * (22) * 2 = 92MB
$$

- We consider fp 16 even with quantised model because this Jetson module supports quantisation till FP16 only (not even bf16): [](https://forums.developer.nvidia.com/t/quantization-in-tensorrt/201227)(“[Quantization in TensorRt](https://forums.developer.nvidia.com/t/quantization-in-tensorrt/201227)”)
1. **Activation Outputs**: The outputs of each transformer layer (activations) are temporarily stored in GPU memory as the model processes the input tokens. Longer or more complex queries might result in larger or more numerous activation outputs, leading to higher GPU memory usage.
2. **Attention Mechanism**: The attention mechanism requires storing the results of attention scores, weighted sums, and other operations in GPU memory during inference. For longer and more complex queries, the size of the attention buffers will be larger.

[*Same conversational query executed with increasing number of expected output tokens.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfXEdAppnpxPrj0LZYI4pnx_YiH_934tGH4mLHDCbJk3BfJt01hj4r-CwQt0D5tgJhsAVS6acFSA8cSxsXcDFgQ6vvemw9hGpIv0IZfLG_cEqxuibOg-gfR3ZMeAihNiCDGwb5g_gcbnyOVPMXN9z0i_nzx?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Same conversational query executed with increasing number of expected output tokens.*

### GPU Memory Usage and Output Length Analysis

To investigate how GPU memory usage changes with increasing output length, we executed a conversational query with a prompt length equal to the median prompt length for the conversational workload category. We selected a conversational query type because TinyLlama is fine-tuned for chat-related tasks. After excluding the warm-up phase, we repeatedly executed the same query, instructing the model to generate progressively longer responses with each run.

Our expectation was that GPU memory usage would increase proportionally with the length of the output. However, an interesting observation emerged: the GPU memory usage remained constant across all five runs, regardless of the output length. Instead, the time spent by the GPU executing the query increased linearly with the increasing output length.

This unexpected result led us to conduct a reverse experiment to further explore the relationship between input and output tokens. We aimed to observe how GPU memory usage would change when varying the number of input tokens while keeping the number of output tokens constant.

[*(Zoomed version) Four task-oriented queries with different prompt lengths executed with the same number of expected output tokens (50).*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcOsjW_RN0UlbC3YSxpAhWUBJVH4ywc4tiFiNyMhCFn2ne3Db5vf5v3xxP3rFJ6bvqHuJm4L42UOgzOFmq8oRJ_iw9qAv1Jo_5bX4I8LmEL4aT-0YIlXgxyyi_dwADXybJcJbeale5LozNbz6r6mODkA60?key=7VKbPIlbOp2wxPlxUTA4Ug)

*(Zoomed version) Four task-oriented queries with different prompt lengths executed with the same number of expected output tokens (50).*

### GPU Memory Usage with Varying Input Tokens

For this experiment, we selected four different queries from the task-oriented workload. Task-oriented queries were chosen over conversational queries due to the greater variation in the number of input tokens available in this workload. The output length was set to 50 tokens for all four queries, and any data from warm-up runs was discarded.

The zoomed-in plot reveals that GPU memory usage does indeed increase as the number of tokens in the prompt increases. While the change in the number of tokens is not orders of magnitude different, there is still a noticeable difference in GPU memory usage, measured in megabytes (MB).

However, the reasoning behind the lack of observed difference in GPU memory usage for varying output tokens in the previous experiment remains unclear. It is possible that the nature of the query being "conversational" may have influenced this outcome.

[*Total query time comparison for similarly sized contextual and task-oriented query.*](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcTzTZiOSvqUz8t83sYJDBk8CjeKy20cZ-LaI_KxmI2iLDJjLtLfaBmfvVvBING5Ub7UqKRlEmfR-ualLDq-0MROmaudV1Gg7LohCAxPyy55oSvUil_kRlak_bmJosiru0aGghdQE8sozGxxvVQUMDlqJ9R?key=7VKbPIlbOp2wxPlxUTA4Ug)

*Total query time comparison for similarly sized contextual and task-oriented query.*

### **Validation of Query Execution Time Relative to Token Count and Query Complexity**

To validate our hypothesis that both the number of tokens in the prompt and the complexity of the query influence the total time required for execution, we compared contextual queries with task-oriented queries. Specifically, we anticipated that contextual queries, which generally involve longer prompts and greater complexity, would take longer to execute than task-oriented queries.

In our workload plot (total time per query vs. total tokens in prompt), we observed that task-oriented queries generally exhibited higher execution times compared to contextual queries, despite the latter typically having longer prompts. This discrepancy was attributed to the difference in the number of output tokens to be generated: 100 tokens for task-oriented queries versus 50 tokens for contextual queries.

To test this further, we selected one query from each workload with similar prompt lengths and set the number of expected output tokens to 50 for both. After discarding warm-up runs, we found that the contextual query indeed took longer to execute than the task-oriented query when the number of expected output tokens was controlled. This finding confirms that, when output token length is held constant, the complexity and context length of the query significantly impact the total execution time, with contextual queries requiring more time than task-oriented ones.