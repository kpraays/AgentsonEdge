recommendation workload

it is a simple workload - using cosine similarity
data from kafka logs - loaded to the machine - not online aggregation
wrapped up in a doceker container


next up:
Cifar-10 training on jetson boards
Shakespeare dataset - https://github.com/TalwalkarLab/leaf
upload te weights to parent pc

get the edge and clienet - internet
running the compute on the board anad sending the boards to pc
get the time btach - to tran  onthe borad - just for reference

recommendation
imagenet - cifar-10

goal for you:
run as many models as possible - only in federated learning setup - measure the time needed to train to a btach

cifar-10
shakespeare
imagenet with limited images

present plots like Zach - figma


project aim: we provide the policy when it good to move the load from edge to cloud. (client -> edge -> cloud)


bert
cifar
imagenet
transformer

##########################################################


# net = VGG('VGG19')
# net = ResNet18()
# net = PreActResNet18()
# net = GoogLeNet()
# net = DenseNet121()
# net = ResNeXt29_2x64d()
# net = MobileNet()
# net = MobileNetV2()
# net = DPN92()
# net = ShuffleNetG2()
# net = SENet18()
# net = ShuffleNetV2(1)
# net = EfficientNetB0()
# net = RegNetX_200MF()
# net = SimpleDLA()
net = LeNet()


Picked cifar-10 dataset and wanted to execute multiple models for it.
Found that each batch was coming to be pretty slow so did not execute and plot all.
Picked LeNet (because noticed Zach had done so).
Trained it on cifar-10 using LeNet and collected the:
	accuracy
	batch number
	epoch
	total time
	time per iteration
I was initially confused regarding what to plot. Last meeting it was said we needed time for each batch as well but then we had multiple epoches. Plotted it against the iterations (17 epochs) - accuracy, step time, total time.
Created the same plots on my PC as reference. (100 epoches)
Board is connected to PC and all, no more power issues.
next up: try more workloads + Federated learning -- more longer running time for the workloads.

###################################

Nov 1, 2023
We care about the data transfer from the edge to the cloud. (edge datacenter to the cloud datacenter)

check gpu usage
double check with a systems too which does not belong to that library: nvidia-smi
run without cuda on cpu (both)
profile it


###################################
get some more context on what they are doing
LLMs+TEEs
FL on the board
run different models on the board and get measurements

Next week Raphael will present.

1. implement those things without a TEE: do encryption, TLS, training, masking of model: without executing in the TEE
2. add TEEs in Rishi's framework

cifar, imagenet:
	know how to do all the steps

to do:
imagenet
mobilebert

time taken for one mini batch for different batch sizes: 8,16,32
forward pass time and backward pass time
add timer inside in the code: hardware counter

change the plots:
plot time vs accuracy
###################################

adam optimiser - batch sized based training

dataset: 1.2 mill images - 315GB

What Zach was doing?
8 minibatch - 8 * 2(2 gpus)
256kb * (4000 - 8 * 2 * 256)

an idea:
Finding the right set of hyperparameters based on your topology - right now, it is grid search (brute force)
Tuning the hyperparameter: as you are going along

iid vs non-iid


what are the right values for the hyperparapmertes for the fastest convergence on resnet 50? (on a particular setting)
depends on topology
hardware